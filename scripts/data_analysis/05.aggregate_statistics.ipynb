{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73823c2b-62d2-4c4b-a724-94493f5bbba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import pathlib\n",
    "import datetime\n",
    "import numpy as np\n",
    "import scipy\n",
    "import easygems.healpix as egh\n",
    "import dask\n",
    "dask.config.set(**{'array.slicing.split_large_chunks': True})\n",
    "import logging\n",
    "from my_library.track_analyses import helpers\n",
    "import pathlib\n",
    "\n",
    "import time\n",
    "import logging\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.perf_counter()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        dt = time.perf_counter() - t0\n",
    "        logging.info(f\"[TIMER] {name}: {dt:.2f} s\")\n",
    "\n",
    "\n",
    "datadir = pathlib.Path(f'/work/bb1153/b382635/plots/tracked_results_2025/dataset_paper/results_data/acp_submission/')\n",
    "outdir = datadir / 'dcc_statisticsV3'\n",
    "os.makedirs(outdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "020b3a55-b012-497f-825e-112853f2f88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baef2aa8-1247-485b-8e9b-cf622e939c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(datadir / 'system_validity.csv', index_col='system_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a329af40-341e-487c-9ff6-f19e5ec9fbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181 that initated above freezing\n",
      "122 that hit the boundary\n",
      "303 that are invalid\n",
      "960 that are valid\n"
     ]
    }
   ],
   "source": [
    "# filter those that hit the boundary or start above freezing\n",
    "invalid_convection = (df.n_cores_above_freezing / df.n_cores) == 1\n",
    "print(invalid_convection.sum(), 'that initated above freezing')\n",
    "# too_complex = df.n_cores > 25\n",
    "# print(too_complex.sum(), 'that are too complex')\n",
    "# too_old = (df.lifetime_mins / 60) > 24 #Â hours\n",
    "# print(too_old.sum(), 'that lived too long')\n",
    "print(df.hits_boundary.sum(), 'that hit the boundary')\n",
    "invalid_rule = df.hits_boundary | invalid_convection\n",
    "invalid = df.index[invalid_rule]\n",
    "print(invalid.size, 'that are invalid')\n",
    "print(df.index.size - invalid.size, 'that are valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ba3a618-2c2f-488b-a8f5-b88c893af5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # new stats\n",
    "# df_old = pd.read_csv(datadir / 'system_validity_prev.csv', index_col='system_id')\n",
    "# all_new_stats = df.index[np.logical_and(np.isnan(df_old.tsteps_before_decay), ~invalid_rule)]\n",
    "# # new_stats = [x for x in stats_to_calc if x not in missing_stats]\n",
    "# data_params = dict(sidx_list = new_stats)\n",
    "# len(new_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bc77baa-8385-4e72-81f4-a44304b42405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate = 1\n",
    "# if iterate:\n",
    "#     batch, size = 23, 50\n",
    "#     new_stats = all_new_stats[50*(batch-21):50*(batch-20)]\n",
    "#     outpath = outdir / f'b{batch}s{size}.nc'\n",
    "# else:\n",
    "#     batch, size = 21, 5\n",
    "#     new_stats = all_new_stats[50*(batch-21):50*(batch-20)]\n",
    "#     outpath = outdir / f'b{batch}s{size}.nc'\n",
    "\n",
    "# data_params = dict(sidx_ignore=invalid, batch=batch, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30ad1b1e-91fc-4f9d-8843-037305155855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select whether to iterate\n",
    "\n",
    "iterate = 1\n",
    "if iterate:\n",
    "    # batch, size = int(sys.argv[1]), int(sys.argv[2])\n",
    "    batch, size = 24, 50\n",
    "    outpath = outdir / f'b{batch}s{size}.nc'\n",
    "    n_clouds = size\n",
    "else:\n",
    "    batch, size = 6, 50\n",
    "    n_clouds = size\n",
    "    outpath = outdir / f'b{batch}s{n_clouds}.nc'\n",
    "    \n",
    "data_params = dict(sidx_ignore=invalid, batch=batch, size=size, n_clouds=n_clouds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a994a27-5427-468a-b94e-5f1279f9f57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate whether complex or isolated multicore DCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3be8eac0-c596-4f6a-84ff-2be1f74900a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "no files to open",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m fdir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/work/bb1153/b382635/data/track_statistics/updraft_ice_only/amazon/system-wise/fcsfirst/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m helpers\u001b[38;5;241m.\u001b[39mload_stats(fdir, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcore_max_w\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata_params)\n",
      "File \u001b[0;32m~/s/my_library/my_library/track_analyses/helpers.py:50\u001b[0m, in \u001b[0;36mload_stats\u001b[0;34m(fdir, variables, n_clouds, batch, size, sidx_list, sidx_ignore, apply)\u001b[0m\n\u001b[1;32m     47\u001b[0m             ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcore\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m,ds\u001b[38;5;241m.\u001b[39mcore\u001b[38;5;241m.\u001b[39msize) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m10000\u001b[39m\u001b[38;5;241m*\u001b[39msidx)\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\u001b[38;5;241m.\u001b[39mexpand_dims(\u001b[38;5;28mdict\u001b[39m(system\u001b[38;5;241m=\u001b[39m[sidx]))\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xr\u001b[38;5;241m.\u001b[39mopen_mfdataset(files, concat_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m'\u001b[39m, combine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnested\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocess\u001b[38;5;241m=\u001b[39mpreprocess)\n",
      "File \u001b[0;32m~/.conda/envs/hackathon_env/lib/python3.11/site-packages/xarray/backends/api.py:1021\u001b[0m, in \u001b[0;36mopen_mfdataset\u001b[0;34m(paths, chunks, concat_dim, compat, preprocess, engine, data_vars, coords, combine, parallel, join, attrs_file, combine_attrs, **kwargs)\u001b[0m\n\u001b[1;32m   1018\u001b[0m paths \u001b[38;5;241m=\u001b[39m _find_absolute_paths(paths, engine\u001b[38;5;241m=\u001b[39mengine, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m paths:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno files to open\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m combine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnested\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(concat_dim, (\u001b[38;5;28mstr\u001b[39m, DataArray)) \u001b[38;5;129;01mor\u001b[39;00m concat_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: no files to open"
     ]
    }
   ],
   "source": [
    "fdir = f'/work/bb1153/b382635/data/track_statistics/updraft_ice_only/amazon/system-wise/fcsfirst/'\n",
    "data = helpers.load_stats(fdir, ['core_max_w'], **data_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9762d11-3c91-4e21-84ad-7e8674acc0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer('isolated'):\n",
    "    # define spatial footprints\n",
    "    core_footprint = data.core_max_w>0\n",
    "    all_footprints = core_footprint.any('core')\n",
    "    core_exists = core_footprint.max(('lat','lon','time'))\n",
    "    # determine if they overlap in space\n",
    "    overlap = (core_footprint.any('time') & (all_footprints & ~core_footprint).any('time')).any(('lat','lon'))\n",
    "    # do all cores overlap? Y -> is isolated\n",
    "    is_isolated_dcc = overlap.where(core_exists).all('core') \n",
    "    n_cores = core_exists.sum('core')\n",
    "    is_isolated_dcc = is_isolated_dcc.where(n_cores>1, 1) # set single-core clouds as isolated\n",
    "    logging.info(f\"assessed core overlaps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253ba136-3e76-449e-947c-c784567ef6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215cbe65-a35d-4d9d-8146-742eb785abc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(ds):\n",
    "    def aggregate_time(d):\n",
    "        vmax = ['cth','core_area','core_depth','anvil_depth','core_max_w','core_th']\n",
    "        vmin = ['abh','core_bh']\n",
    "        for v in vmax:\n",
    "            d[v] = d[v].max('time')\n",
    "        for v in vmin:\n",
    "            d[v] = d[v].min('time')\n",
    "        return d\n",
    "    ds['anvil_depth'] = ds.anvil_depth.max(('lat','lon'))\n",
    "    ds['abh'] = ds.abh.min(('lat','lon'))\n",
    "    ds['core_area'] = ds.core_area.max('level_full') if 'level_full' in ds.core_area.dims else ds.core_area\n",
    "    ds['core_depth'] = ds.core_depth.max(('lat','lon')) if 'lat' in ds.core_depth.dims else ds.core_depth\n",
    "    ds['core_max_w'] = ds.core_max_w.max(('lat','lon')) if 'lat' in ds.core_max_w.dims else ds.core_max_w\n",
    "    ds['core_pr'] = ds.core_pr.max('level_full')\n",
    "    ds['cloud_pr'] = (ds.cloud_pr * 60*15).sum('time') # to kg m^-2\n",
    "    ds['core_pr'] = (ds.core_pr * 60*15).sum('time') # to kg m^-2\n",
    "    ds = aggregate_time(ds)\n",
    "    return ds\n",
    "\n",
    "# load\n",
    "agg_vars = ['cloud_area','cth','cloud_depth','core_area','cloud_pr','core_pr',\n",
    "            'core_depth','anvil_depth','core_max_w','abh','core_th','core_bh',\n",
    "           ]\n",
    "with timer('load stats'):\n",
    "    ds = helpers.load_stats(fdir, agg_vars, apply=func, **data_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2892f40-8dad-48aa-b6b8-4366035cda70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert units\n",
    "ds['cloud_area'] = ds['cloud_area'] / (1000**2)\n",
    "ds['core_area'] = ds['core_area'] / (1000**2)\n",
    "ds['cloud_depth'] = ds['cloud_depth'] / (1000)\n",
    "ds['anvil_depth'] = ds['anvil_depth'] / (1000)\n",
    "ds['core_depth'] = ds['core_depth'] / (1000)\n",
    "ds['cth'] = ds['cth'] / (1000)\n",
    "ds['abh'] = ds['abh'] / (1000)\n",
    "ds['core_th'] = ds['core_th'] / (1000)\n",
    "ds['core_bh'] = ds['core_bh'] / (1000)\n",
    "logging.info(f\"got bulk statistics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8f24da-7288-4dd2-a795-e1a3d7e0478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3550cc06-14c1-49f0-89ad-b53b88c1b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - n cores\n",
    "path = '/work/bb1153/b382635/data/final_tracks/updraft_ice_only/amazon/data_filtering_stats/system_n_cores.csv'\n",
    "ncores = pd.read_csv(path, index_col='system_id').rename_axis('system')\n",
    "\n",
    "# - ABHs\n",
    "path = '/work/bb1153/b382635/data/final_tracks/updraft_ice_only/amazon/data_filtering_stats/system_anvil_base_height.csv'\n",
    "abh = pd.read_csv(path, index_col='system_id').rename_axis('system')\n",
    "\n",
    "# - lifetime\n",
    "path = '/work/bb1153/b382635/data/final_tracks/updraft_ice_only/amazon/data_filtering_stats/system_lifetime.csv'\n",
    "lifetime = pd.read_csv(path, index_col='system_id').rename_axis('system')\n",
    "\n",
    "# to dataset\n",
    "abh = xr.Dataset.from_dataframe(abh).rename({'0':'ABH'}).round()\n",
    "lifetime['0'] = pd.to_timedelta(lifetime['0'])\n",
    "lifetime = xr.Dataset.from_dataframe(lifetime).rename({'0':'lifetime'})\n",
    "ncores = xr.Dataset.from_dataframe(ncores)\n",
    "metadata = xr.Dataset({'ABH':abh.ABH, 'lifetime':lifetime.lifetime, 'ncores':ncores.n_cores})\n",
    "\n",
    "# collect metadata\n",
    "mds = metadata.sel(system=ds.system)\n",
    "ds['n_cores'] = mds.ncores\n",
    "ds['lifetime'] = mds.lifetime\n",
    "\n",
    "# safe time storage\n",
    "ds['lifetime'] = (ds[\"lifetime\"] / np.timedelta64(1, \"s\")).astype(\"float32\") # to seconds\n",
    "ds.lifetime.attrs = dict(units='seconds')\n",
    "logging.info(f\"collected metatdata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2eb22e-bcd3-4b3a-9a73-45792683558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign whether or not the DCC is isolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90663cf6-a6fa-43c3-a52f-1edd1c62d3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['is_isolated'] = is_isolated_dcc\n",
    "ds['is_isolated'] = ds.is_isolated.where(ds.n_cores<4, 0) # force systems with >3 cores to be 'complex'\n",
    "logging.info(f\"assigned whether complex or isolated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ea1e4d-cc19-4173-aa8f-255dc3c9bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise time by lifetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ce9bc9-d2ac-4d3c-bb91-02a7ad738d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer('by lifetime'):\n",
    "    lifetime_stats = ['cloud_area','cloud_depth']\n",
    "    bulk_stats = [x for x in ds.data_vars if x not in ['cloud_area','cloud_depth']]\n",
    "    subset = ds[lifetime_stats]\n",
    "    # subset = subset.drop_vars(['lat','lon','level_full'])\n",
    "    obj_exists = subset.cloud_area>0\n",
    "    life = helpers.normalise_by_lifetime(obj_exists, [subset.cloud_area, subset.cloud_depth], )\n",
    "    init_t = (subset.cloud_area>0).idxmax('time').compute()\n",
    "    life['TOI'] = init_t\n",
    "    logging.info(f\"normalised by lifetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7e3b3d-6e0d-4f32-8896-b8b984ef4781",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"saving...\")\n",
    "final = xr.merge([ds[bulk_stats], life])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353d2619-b957-4c5a-a3af-a09cdc6efdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9ccfdb-0bec-4e46-8859-256e50712388",
   "metadata": {},
   "outputs": [],
   "source": [
    "final[\"TOI\"].attrs = {}\n",
    "final[\"TOI\"].encoding = {\n",
    "    \"units\": \"seconds since 1970-01-01 00:00:00\",\n",
    "    \"calendar\": \"proleptic_gregorian\",\n",
    "    \"dtype\": \"int64\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4266ddef-b4ea-4fbf-aa40-9263b2575963",
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer('save'):\n",
    "    final.to_netcdf(outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481ca28a-2ff0-445f-b82f-7e6ddc0f556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084bff2e-8db6-400a-9430-642922a6d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f04e9e-de66-44de-8360-7fe7b8a39d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ccc58f-23f3-4442-b4d3-351865de1f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731d145a-88e7-446b-a8bf-219b3e4934e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c62069d-aff5-46fc-b3b3-1e50c33ccdd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "hackathon_env_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
